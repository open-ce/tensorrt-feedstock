{% set tensorrt_version = "7.2.0.13" %}                                                  #[ppc64le]
{% set sha256 = "32ea84fb87619ec6b90ca3f4bb9d9dd3d5ca12f6a5f2db9777bfc858d9e62464" %}    #[ppc64le]
{% set trt_tar = "TensorRT-7.2.0.13.CentOS-8.1.ppc64le-gnu.cuda-11.0.cudnn8.0.tar.gz" %} #[ppc64le]

{% set tensorrt_version = "7.2.1.6" %}                                                  #[x86_64]
{% set sha256 = "3d42890424e2a2a155df67aaace45d001a95f12d6781008c5dcf62ea480ea664" %}   #[x86_64]
{% set trt_tar = "TensorRT-7.2.1.6.CentOS-7.6.x86_64-gnu.cuda-11.0.cudnn8.0.tar.gz" %}  #[x86_64]

{% set trt_tar_dir =  environ.get('LOCAL_SRC_DIR', 0) %}

package:
  name: tensorrt
  version: {{ tensorrt_version }}

source:
  - url: file:/{{ trt_tar_dir }}/{{ trt_tar }}
    sha256: {{ sha256 }}                        
    patches:
       # 02xx - GPU only patch specific to open-ce (maybe)
       - 0204-update-Makefile.conf-for-conda-builds.patch
       - 0205-update-default-data-path.patch

       # 03xx - patch temporary to fix a problem that when fixed upstream can be removed
       - 0303-fix-yolov3-for-py3x.patch 
    folder: tensorrt

  - git_url: "https://github.com/NVIDIA/TensorRT.git"
    git_tag: 7.2.1
    patches:
       # 02xx - GPU only patch specific to open-ce (maybe)
       - 0201-add-build-onnx2trt-flag.patch
       - 0202-only-install-onnx2trt.patch
       - 0203-Fixed-samples-CMakeLists-for-conda-builds.patch

       # 03xx - patch temporary to fix a problem that when fixed upstream can be removed
       - 0301-fix-GLIBC_2.14-link-issue-with-libnvinfer.so.patch # [x86_64]
       - 0302-Fix-libcaffeparser-name-for-samples.patch
       - 0304-modified-print-function-syntax-as-per-py3.patch
       #- add-makefiles-for-new-samples.patch
    folder: TensorRT

build:
  number: 4
  string: h{{ PKG_HASH }}_cuda{{ cudatoolkit | replace(".*", "") }}_py{{ python | replace(".", "") }}_pb{{ protobuf | replace(".*", "")}}_{{ PKG_BUILDNUM }}
  activate_in_script: True
  script_env:
    - CUDA_HOME

outputs:
   - name: tensorrt
     script: build-tensorrt.sh
     requirements:
         build:
           - {{ compiler('cxx') }}
           - cmake {{ cmake }}
           - make
         host:
           - python {{ python }}
           - protobuf {{ protobuf }}
           - libprotobuf {{ protobuf }}
           - cudnn {{ cudnn }}
           - libmemcpy_wrapper            #[x86_64]
           - libclock_gettime_wrapper     #[x86_64]

         run:
           - python {{ python }}
           - cudatoolkit {{ cudatoolkit }}
           - protobuf {{ protobuf }}
           - libprotobuf {{ protobuf }}
           - cudnn {{ cudnn }}
           - numpy {{ numpy }}
           - uff {{ uff }}
           - graphsurgeon {{ graphsurgeon }}
           - onnx-graphsurgeon {{ onnx_graphsurgeon }}
           - libmemcpy_wrapper            #[x86_64]
           - libclock_gettime_wrapper     #[x86_64]

   - name: tensorrt-samples
     script: build-tensorrt-samples.sh
     requirements:
         build:
           - {{ compiler('cxx') }}
           - cmake {{ cmake }}
           - make
         host:
           - python {{ python }}
           - protobuf {{ protobuf }}
           - libprotobuf {{ protobuf }}
           - cudnn {{ cudnn }}
           - tensorrt {{ tensorrt_version }}

         run:
           - decorator
           - pywget
           - tensorrt {{ tensorrt_version }}
           - python {{ python }}
           - onnx {{ onnx }}
           - pillow {{ pillow }}
           - requests {{ requests }}
           - cmake {{ cmake }}
           - make
           - numpy {{ numpy }}
           - gxx_linux-64       {{ cxx_compiler_version }} # [x86_64]
           - gxx_linux-ppc64le  {{ cxx_compiler_version }} # [ppc64le]

about:
  home: https://developer.nvidia.com/tensorrt
  license: NVIDIA Software License, Apache-2.0
  summary: TensorRT is a C++ library for high performance inference on NVIDIA GPUs and deep learning accelerators.
  description: |
    NVIDIA TensorRT is a platform for high-performance deep learning inference. 
    It includes a deep learning inference optimizer and runtime that delivers low 
    latency and high-throughput for deep learning inference applications.
  dev_url: https://github.com/NVIDIA/TensorRT/
  doc_url: https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html

extra:
  recipe-maintainers:
    - open-ce/open-ce-dev-team
